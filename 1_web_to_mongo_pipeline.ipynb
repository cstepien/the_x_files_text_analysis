{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Transcripts of Every X-Files Episode and Send to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build functions for web scraper to MongoDB pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_add some text here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scraper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape the page with the episode transcript links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"Request successful\\n\")\n",
    "    else:\n",
    "        print(\"Error code\\n\", response.status_code)\n",
    "    \n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page,\"lxml\")\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a dictionary of episode transcript links, episode number, and season number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_link_dict(soup):\n",
    "    \n",
    "    episode_list = []\n",
    "    episode_num = 1\n",
    "    \n",
    "    seasons = soup.find_all(class_=\"fusion-text\")[1:]\n",
    "    \n",
    "    for season_ix,season in enumerate(seasons):\n",
    "        for episode in season.find_all('a'):\n",
    "            document = {}\n",
    "            document['episode_number'] = episode_num\n",
    "            document['season'] = season_ix+1\n",
    "            document['episode_name'] = episode.text\n",
    "            document['link'] = episode['href']\n",
    "            episode_list.append(document)\n",
    "            episode_num+=1\n",
    "            \n",
    "    return episode_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(episode):\n",
    "    soup = get_soup(episode['link'])\n",
    "    episode['transcript'] = soup.find('p').getText()\n",
    "    \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Scully & Mulder lines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lines(document):\n",
    "    regex_scully = '\\n\\nSCULLY: (.*)'\n",
    "    regex_mulder = '\\n\\nMULDER: (.*)'\n",
    "    parens_regex = '[\\(].*?[\\)]'\n",
    "    \n",
    "    # remove all text inside parentheses - they are stage directions, not dialogue\n",
    "    dialogue = re.sub(parens_regex, '', document['transcript'])\n",
    "    scully_lines = re.findall(regex_scully, dialogue)\n",
    "    mulder_lines = re.findall(regex_mulder, dialogue)\n",
    "    document['scully_lines'] = scully_lines\n",
    "    document['mulder_lines'] = mulder_lines\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add document to MongoDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_mongodb(document, client_connection):\n",
    "    \n",
    "    \"\"\"Takes a dictionary in document form and sends it to a pre-specified database\n",
    "    and collection in mongodb. document is the document to enter into the database. \n",
    "    client_connection is an already opened connection with a MongoDB client pointing\n",
    "    to a specified location.\"\"\"\n",
    "    \n",
    "    mongo_result = client_connection.insert_one(document)\n",
    "    \n",
    "    if not mongo_result.acknowledged: \n",
    "        raise ValueError(\"Failed to add document to MongoDB. Check connection and document.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web to MongoDB pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_to_mongo(url, database, collection):\n",
    "    \n",
    "    print(\"Connecting to Mongo database...\\n\")\n",
    "    client = MongoClient()\n",
    "    db = client[database]\n",
    "    mongo_loc = db[collection]\n",
    "    \n",
    "    print(\"Scraping episode meta-data...\\n\")\n",
    "    link_soup = get_soup(url)\n",
    "    episode_list = make_link_dict(link_soup)\n",
    "    \n",
    "    for episode in episode_list:\n",
    "        print(\"Scraping transcript...\\n\")\n",
    "        episode_with_transcript = get_transcript(episode)\n",
    "        print(\"Cleaning and sending to Mongo...\\n\")\n",
    "        cleaned_episode = get_lines(episode_with_transcript)\n",
    "        send_to_mongodb(cleaned_episode, mongo_loc)\n",
    "        seconds = random.uniform(0,5)\n",
    "        print(f\"Sleeping {int(seconds)} seconds until episode {episode['episode_number']}.\\n\")\n",
    "        time.sleep(seconds)\n",
    "\n",
    "    client.close()\n",
    "    print('Done.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get, clean and store the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://scifi.media/x-files/transcripts/'\n",
    "web_to_mongo(url, 'x-files', 'transcripts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The links for all the transcripts are just the episode number at the end of the link - could simpify the code to create a link list from that, but then we wouldn't get the season and episode name as easily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error messages and other production aspects to add "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* docstrings\n",
    "* error messages/type checking\n",
    "* for `get_lines()` do some more checking on the input - maybe allow for checking with the user if they mean dana scully, or fox mulder, or any other character. \n",
    "* add check db size for `web_to_mongo`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
